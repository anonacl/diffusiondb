{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2964682d-c51c-4578-9d8c-7d86026ff46d",
   "metadata": {},
   "source": [
    "Setup the environment, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b346c5-ef13-47ec-a2fe-35fa0bf6ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the following with your specific version of CUDA, if any. \n",
    "# !pip install torch --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install h5py pandas numpy matplotlib diffusers transformers scipy ftfy pyarrow regex wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf1fca-7e69-47da-8abf-cae71f24d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud as wc\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb974068-6ff1-4acf-bcd5-0cfd8be61268",
   "metadata": {},
   "source": [
    "Load the pipeline to get the same tokenizer used as Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6772927-0705-44d6-87c8-2aed803f7200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# auth_token = os.environ[\"HFTOKEN\"]\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56133865-94ea-483e-9331-0e012bac839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_parquet(\n",
    "    './indexes/metadata-large.parquet',\n",
    "    columns=['prompt']\n",
    ")['prompt']\n",
    "print(\"Length of prompts: \", len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61572e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(set(prompts))\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfae87-03ea-4736-b86c-75c48781550a",
   "metadata": {},
   "source": [
    "## Prompt uniqueness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bd4ba-0698-4366-9fd5-5dcb1d1d19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprompts = set(list(prompts.prompt))\n",
    "\n",
    "# Get count of each prompt\n",
    "ct_dict = {k:0 for k in sprompts}\n",
    "for k in prompts.prompt:\n",
    "    ct_dict[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0a369-1dd4-42cd-869a-ff117fb1879d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([v for v in ct_dict.values()])\n",
    "cts, bins = np.histogram(x, bins=np.unique(x))\n",
    "\n",
    "plt.bar(bins[:-1], cts)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"N (number of times a prompt appears in the dataset)\")\n",
    "plt.ylabel(\"Number of prompts that appear N times\")\n",
    "plt.title(\"How unique are the prompts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b01bf2-1377-4fe0-888a-a39859471eb4",
   "metadata": {},
   "source": [
    "# Prompt lengths?\n",
    "\n",
    "By **specifier clauses** and **token length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e726200-822a-4dcb-907a-facd3a6a9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose separators, find token ids of that token\n",
    "sep_ids = [\",\", \";\", \"|\"]\n",
    "for s in sep_ids:\n",
    "    print(f\"sep_id: '{s}': \", pipe.tokenizer.encode(f\"know{s}nothing\")[2:-2]) # ids of separator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fa378-d5bc-41e8-8c28-17653c6b8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tok_length(text_inputs):\n",
    "    \"\"\"Calculate average number of tokens in input\"\"\"\n",
    "    n_tokens = text_inputs[\"attention_mask\"].sum(-1) - 2 # remove BOS and EOS added tags\n",
    "    return n_tokens\n",
    "\n",
    "def batch_concepts(prompt:str):\n",
    "    \"\"\"Return the concepts in each prompt as strings\"\"\"\n",
    "    concepts = re.split(';|,|\\|', prompt)\n",
    "    return concepts\n",
    "\n",
    "def batch_num_concepts(text_inputs):\n",
    "    \"\"\"Calculate how many concepts are in the prompt, from the tokens\"\"\"\n",
    "    split_ids = [267, 282, 347] # comma, semicolon, pipe\n",
    "    iids = text_inputs[\"input_ids\"]\n",
    "    mask = torch.zeros_like(iids)\n",
    "    seps = [torch.eq(iids, sid) for sid in split_ids]\n",
    "    for s in seps:\n",
    "        mask = torch.logical_or(mask, s)\n",
    "    out = mask.sum(-1) + 1 # Number of concepts = number of separators + 1\n",
    "    return out\n",
    "\n",
    "def tok_frequencies(text_inputs):\n",
    "    \"\"\"Calculate the frequency each token appears in a batch of tokenized inputs\"\"\"\n",
    "    iids = text_inputs[\"input_ids\"]\n",
    "    ids, counts = torch.unique(iids, return_counts=True)\n",
    "    return ids, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = np.array(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ec951-88de-4256-b4e5-c3c510ffe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Long running cell. Choose batch size that computer can handle easily\n",
    "bs = 10000\n",
    "i = 0\n",
    "vocab_size = pipe.tokenizer.vocab_size\n",
    "tokfreqs = torch.zeros(vocab_size, dtype=torch.int64)\n",
    "total_nconcepts = torch.zeros(len(prompts), dtype=torch.int16)\n",
    "total_token_length = torch.zeros(len(prompts), dtype=torch.int16)\n",
    "total_iter = len(prompts) // bs + 1\n",
    "nprompts = len(prompts)\n",
    "n = 0\n",
    "with tqdm(total=total_iter) as pbar:\n",
    "    while i < nprompts:\n",
    "        n+= 1\n",
    "        pbar.update(1)\n",
    "        pidxs = slice(i, i+bs)\n",
    "        p = prompts[pidxs].tolist()\n",
    "        text_inputs = pipe.tokenizer(\n",
    "            p,\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipe.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        nconcepts = batch_num_concepts(text_inputs)\n",
    "        total_nconcepts[pidxs] = nconcepts\n",
    "        length = batch_tok_length(text_inputs)\n",
    "        total_token_length[pidxs] = length\n",
    "\n",
    "        ids, counts = tok_frequencies(text_inputs)\n",
    "        tokfreqs[ids] += counts\n",
    "        i += bs\n",
    "\n",
    "        if n == total_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d5674-7cce-4608-b00d-8ab5afaa9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.grid(alpha=0.2)\n",
    "n = n.astype(\"int\")\n",
    "n, bins, patches = plt.hist(np.array(total_token_length), bins=37, edgecolor='white', linewidth=0.5, alpha=0.9)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"Number of tokens in prompt\", fontsize=16)\n",
    "plt.title(\"Distribution of Prompt Length (# of Tokens)\", fontsize=18)\n",
    "plt.savefig(\"plots/token_length_dist.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745644a-c77d-4804-bfa0-2beb3369f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.grid(alpha=0.2)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "n, bins, patches = plt.hist(np.array(total_nconcepts), bins=range(0, np.unique(total_nconcepts).max().item()+1), edgecolor='#e0e0e0', linewidth=0.5, alpha=0.9)\n",
    "plt.xlabel(\"Number of specifier clauses in prompt\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "ticks = list(range(0, total_nconcepts.max().item(), 5)); ticks[0]=1\n",
    "plt.xticks(ticks=ticks)\n",
    "plt.title(\"Distribution of Prompt Length by Specifier Clause\", fontsize=18)\n",
    "plt.savefig(\"plots/spec_clause_length.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91855d7-4f67-4e11-973d-af21cf5a4191",
   "metadata": {},
   "source": [
    "## Concept Frequency\n",
    "\n",
    "A qualitative analysis of the concepts present in DiffusionDB. We manually filter the top tokens for stop words, combining subtoken representations into meaningful concepts, before displaying in a WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e56fc1-1e08-44ca-832a-6ccfa763ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top K tokens in the corpus, visually filter as needed\n",
    "cts, idxs = tokfreqs.topk(k=100)\n",
    "print(\"\\n\".join([\" :: \".join((str(pipe.tokenizer._convert_id_to_token(idx.item())), str(cts[i].item()))) for i, idx in enumerate(idxs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3be00-3672-48d8-b773-3f36fbf34abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered and combined tokens\n",
    "words = {\n",
    "  \"art\": 784355,\n",
    "  \"detailed\": 714959,\n",
    "  \"artstation\": 476150,\n",
    "  \"painting\": 438349,\n",
    "  \"portrait\": 399555,\n",
    "  \"realistic\": 365993,\n",
    "  \"8k\": 323039,\n",
    "  \"highly\": 319087,\n",
    "  \"lighting\": 310602,\n",
    "  \"digital\": 295669,\n",
    "  \"intricate\": 276934,\n",
    "  \"beautiful\": 276268,\n",
    "  \"concept\": 256254,\n",
    "  \"trending\": 245511,\n",
    "  \"style\": 235599,\n",
    "  \"4k\": 235164,\n",
    "  \"cinematic\": 229357,\n",
    "  \"sharp\": 228603,\n",
    "  \"greg rutkowski\": 222008,\n",
    "  \"render\": 221661,\n",
    "  \"illustration\": 221422,\n",
    "  \"focus\": 210662,\n",
    "  \"high\": 188288,\n",
    "  \"fantasy\": 177511,\n",
    "  \"octane\": 176801,\n",
    "  \"face\":162641,\n",
    "  \"photo\":161967,\n",
    "  \"light\": 155787,\n",
    "  \"black\": 131100,\n",
    "  \"wearing\": 130106,\n",
    "  \"dark\": 124368,\n",
    "  \"smooth\": 120759,\n",
    "  \"white\": 119682,\n",
    "  \"hyper\": 117479,\n",
    "  \"unreal engine\": 114896,\n",
    "  \"background\": 114650,\n",
    "  \"elegant\": 111326,\n",
    "  \"hair\": 110355,\n",
    "  \"full\": 109023,\n",
    "  \"mucha\": 105940,\n",
    "  \"hyper\": 107780,\n",
    "}\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc38559-c71b-42fb-b830-db135c46f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = wc.WordCloud(width=500, height=300, background_color=\"white\", min_font_size=10, relative_scaling=0.0001, colormap=\"Dark2\").fit_words(words)\n",
    "im = cloud.to_svg(True)\n",
    "\n",
    "with open('./plots/wordcloud.svg', 'w') as fp:\n",
    "    fp.write(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea09594-4fa9-4437-9389-d0fbd755734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im.save(\"plots/wordcloud_freqs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "029fbd778f47b5c00e01206581620321254d0338aac98a024144250b6baa6a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
